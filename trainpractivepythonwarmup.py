# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O5ctA51nr99OS_uoO_qen5DRBazCLW-6

1. Handle the data on dependents coulmn
"""

!pip install lazypredict

import pandas as pd
import numpy as np
import lazypredict
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder,StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from lazypredict.Supervised import LazyClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
label_encoder = LabelEncoder()
df = pd.read_csv('/content/train.csv')
df.head() #Display the rows

"""Missing Values and date cleaning"""

df.isnull().sum()

# Check for missing values in each column
# missing_values = df.isnull().sum()
# print("Missing values in each column:")
# print(missing_values)

# Save the cleaned data to a new CSV file
## df.to_csv('/content/train.csv', index=False)

for column in df.columns:
    # Check if there are any missing values in the column
    if df[column].isnull().sum() > 0:
        # Calculate the mode of the column
        mode_value = df[column].mode()[0]
        # Replace missing values with the mode
        df[column].fillna(mode_value, inplace=True)
df.head() #Display the rows
##df.value_counts()
df['Dependents'] = label_encoder.fit_transform(df['Dependents'])
df.head()

# df.isnull().sum()

#Check for missing values in each column
missing_values = df.isnull().sum()
print("Missing values in each column:")
print(missing_values)

target_column = 'Loan_Status'  # Replace with your target variable
class_distribution = df[target_column].value_counts()
print(f"Class distribution in '{target_column}':")
print(class_distribution)

# Visualize the distribution
class_distribution.plot(kind='bar')
plt.title(f'Class Distribution in {target_column}')
plt.xlabel(target_column)
plt.ylabel('Frequency')
plt.show()

# Identify categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns
# Apply label encoding to categorical columns
label_encoders = {}
for column in categorical_columns:
    label_encoder = LabelEncoder()
    df[column] = label_encoder.fit_transform(df[column])
    label_encoders[column] = label_encoder

# Standardization
scaler_standard = StandardScaler()
df_standardized = df.copy()
df_standardized[df.columns] = scaler_standard.fit_transform(df)

# Min-Max Normalization
scaler_minmax = MinMaxScaler()
df_normalized = df.copy()
df_normalized[df.columns] = scaler_minmax.fit_transform(df)

# Identify features and target variable
X = df.drop('Loan_Status', axis=1)  # Replace 'Loan_Status' with your target variable
y = df['Loan_Status']  # Replace 'Loan_Status' with your target variable

# Apply label encoding to the target variable if it's categorical
if y.dtype == 'object':
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)
# Apply label encoding to categorical columns in features
categorical_columns = X.select_dtypes(include=['object']).columns
for column in categorical_columns:
    label_encoder = LabelEncoder()
    X[column] = label_encoder.fit_transform(X[column])

# Standardization (if needed)
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Lazy Predict using LazyClassifier
clf = LazyClassifier(predictions=True)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# Initialize lists for metrics
model_names = []
accuracies = []
precisions = []
recalls = []
f1_scores = []

# Iterate over models and predictions
for model_name, y_pred in predictions.items():
    # Check if predictions exist
    if y_pred is not None:
        # Compute evaluation metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        # Append metrics to lists
        model_names.append(model_name)
        accuracies.append(accuracy)
        precisions.append(precision)
        recalls.append(recall)
        f1_scores.append(f1)

# Create a DataFrame for metrics
metrics_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1-score': f1_scores
})

# Reshape the DataFrame for Seaborn plotting
metrics_df = pd.melt(metrics_df, id_vars='Model', var_name='Metric', value_name='Score')

# Plotting with Seaborn
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df, palette='muted')
plt.title('Model Performance Metrics')
plt.xlabel('Models')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

